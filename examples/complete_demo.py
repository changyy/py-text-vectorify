#!/usr/bin/env python3
"""
å¤šå±‚å‘é‡åŒ–ç³»ç»Ÿå®Œæ•´ä½¿ç”¨ç¤ºä¾‹
Multi-layer Vectorization System Complete Usage Example

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨å¤šå±‚å‘é‡åŒ–ç³»ç»Ÿæ¥å¤„ç†ä¸­æ–‡æ–‡æœ¬ï¼ŒåŒ…æ‹¬ï¼š
1. TF-IDF ç»Ÿè®¡ç‰¹å¾æå–
2. ä¸»é¢˜å»ºæ¨¡ï¼ˆä¸»é¢˜åˆ†å¸ƒå‘é‡ï¼‰
3. å¤šå±‚èåˆç­–ç•¥
4. ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†ï¼ˆjiebaåˆ†è¯ï¼‰
"""

import json
import logging
import numpy as np
from pathlib import Path
from text_vectorify.factory import EmbedderFactory

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def demo_basic_usage():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("åŸºç¡€ä½¿ç”¨ç¤ºä¾‹ - å•ä¸ªåµŒå…¥å™¨")
    print("=" * 60)
    
    # 1. TF-IDF åµŒå…¥å™¨
    print("\n1. TF-IDF åµŒå…¥å™¨ (æ”¯æŒä¸­æ–‡åˆ†è¯)")
    tfidf_embedder = EmbedderFactory.create_embedder(
        'TFIDFEmbedder',
        max_features=200,
        chinese_tokenizer='jieba',
        ngram_range=(1, 2),
        min_df=1,
        max_df=0.95
    )
    
    # ä¸­æ–‡æ–‡æœ¬æ ·æœ¬
    chinese_texts = [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹çš„å¼€å‘ã€‚",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å¸®åŠ©è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
        "è®¡ç®—æœºè§†è§‰è®©æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œåˆ†æå›¾åƒä¸è§†é¢‘å†…å®¹ã€‚",
        "æ•°æ®ç§‘å­¦ç»“åˆç»Ÿè®¡å­¦ã€æ•°å­¦å’Œè®¡ç®—æœºç§‘å­¦æ¥ä»æ•°æ®ä¸­æå–æ´å¯Ÿã€‚"
    ]
    
    # ç¼–ç æ–‡æœ¬
    tfidf_vectors = tfidf_embedder.encode(chinese_texts)
    print(f"TF-IDF å‘é‡ç»´åº¦: {len(tfidf_vectors[0])}")
    print(f"å¤„ç†æ–‡æ¡£æ•°é‡: {len(tfidf_vectors)}")
    
    # 2. ä¸»é¢˜åµŒå…¥å™¨
    print("\n2. ä¸»é¢˜åµŒå…¥å™¨ (LDA)")
    topic_embedder = EmbedderFactory.create_embedder(
        'TopicEmbedder',
        n_topics=10,
        method='lda',
        language='chinese'
    )
    
    # ä½¿ç”¨æ›´å¤šæ–‡æœ¬æ¥è®­ç»ƒä¸»é¢˜æ¨¡å‹
    topic_texts = chinese_texts + [
        "Python æ˜¯æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ çš„çƒ­é—¨ç¼–ç¨‹è¯­è¨€ã€‚",
        "TensorFlow å’Œ PyTorch æ˜¯æµè¡Œçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚",
        "æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ æµç¨‹ä¸­çš„å…³é”®æ­¥éª¤ã€‚",
        "ç‰¹å¾å·¥ç¨‹å¯¹æ¨¡å‹æ€§èƒ½æœ‰é‡è¦å½±å“ã€‚",
        "äº¤å‰éªŒè¯æ˜¯è¯„ä¼°æ¨¡å‹æ³›åŒ–èƒ½åŠ›çš„é‡è¦æ–¹æ³•ã€‚",
        "ç¥ç»ç½‘ç»œçš„ä¼˜åŒ–é€šå¸¸ä½¿ç”¨æ¢¯åº¦ä¸‹é™ç®—æ³•ã€‚"
    ]
    
    topic_vectors = topic_embedder.encode(topic_texts)
    print(f"ä¸»é¢˜å‘é‡ç»´åº¦: {len(topic_vectors[0])}")
    print(f"å¤„ç†æ–‡æ¡£æ•°é‡: {len(topic_vectors)}")
    
    # æ˜¾ç¤ºä¸»é¢˜ä¿¡æ¯
    topics = topic_embedder.get_topics()
    print(f"\nå‘ç°çš„ä¸»é¢˜æ•°é‡: {len(topics)}")
    for i, topic in enumerate(topics[:3]):  # æ˜¾ç¤ºå‰3ä¸ªä¸»é¢˜
        print(f"ä¸»é¢˜ {topic['topic_id']}: {topic['representation']}")
    
    return tfidf_embedder, topic_embedder

def demo_multi_layer_fusion():
    """å¤šå±‚èåˆç­–ç•¥ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("å¤šå±‚èåˆç­–ç•¥ç¤ºä¾‹")
    print("=" * 60)
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›ã€‚",
        "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥å¸®åŠ©åŒ»ç”Ÿè¿›è¡Œç–¾ç—…è¯Šæ–­ã€‚",
        "æ·±åº¦å­¦ä¹ åœ¨åŒ»å­¦å½±åƒåˆ†æä¸­è¡¨ç°å‡ºè‰²ã€‚",
        "æ•°æ®ç§‘å­¦å®¶ä½¿ç”¨ç»Ÿè®¡æ–¹æ³•åˆ†æåŒ»ç–—æ•°æ®ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†å¯ä»¥ç†è§£åŒ»å­¦æ–‡çŒ®å’Œç—…å†ã€‚"
    ]
    
    # 1. è¿æ¥èåˆç­–ç•¥
    print("\n1. è¿æ¥èåˆç­–ç•¥ (Concatenation)")
    concat_config = [
        {
            'name': 'tfidf_layer',
            'type': 'TFIDFEmbedder',
            'config': {
                'max_features': 100,
                'chinese_tokenizer': 'jieba',
                'ngram_range': (1, 2)
            }
        },
        {
            'name': 'topic_layer',
            'type': 'TopicEmbedder',
            'config': {
                'n_topics': 8,
                'method': 'lda',
                'language': 'chinese'
            }
        }
    ]
    
    concat_embedder = EmbedderFactory.create_embedder(
        'MultiLayerEmbedder',
        embedder_configs=concat_config,
        fusion_method='concat',
        normalize=True
    )
    
    concat_vectors = concat_embedder.encode(test_texts)
    print(f"è¿æ¥èåˆå‘é‡ç»´åº¦: {len(concat_vectors[0])}")
    
    # 2. åŠ æƒèåˆç­–ç•¥
    print("\n2. åŠ æƒèåˆç­–ç•¥ (Weighted)")
    weighted_embedder = EmbedderFactory.create_embedder(
        'MultiLayerEmbedder',
        embedder_configs=concat_config,
        fusion_method='weighted',
        weights=[0.6, 0.4],  # TF-IDFæƒé‡0.6ï¼Œä¸»é¢˜æƒé‡0.4
        normalize=True
    )
    
    weighted_vectors = weighted_embedder.encode(test_texts)
    print(f"åŠ æƒèåˆå‘é‡ç»´åº¦: {len(weighted_vectors[0])}")
    
    # 3. æ³¨æ„åŠ›èåˆç­–ç•¥
    print("\n3. æ³¨æ„åŠ›èåˆç­–ç•¥ (Attention)")
    attention_embedder = EmbedderFactory.create_embedder(
        'MultiLayerEmbedder',
        embedder_configs=concat_config,
        fusion_method='attention',
        normalize=True
    )
    
    attention_vectors = attention_embedder.encode(test_texts)
    print(f"æ³¨æ„åŠ›èåˆå‘é‡ç»´åº¦: {len(attention_vectors[0])}")
    
    return concat_embedder, weighted_embedder, attention_embedder

def demo_config_file_usage():
    """é…ç½®æ–‡ä»¶ä½¿ç”¨ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("é…ç½®æ–‡ä»¶ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    config_path = Path("configs/multi_layer_simple.json")
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config_data = json.load(f)
        
        print(f"ä»é…ç½®æ–‡ä»¶åŠ è½½: {config_path}")
        print(f"é…ç½®å†…å®¹: {json.dumps(config_data, indent=2, ensure_ascii=False)}")
        
        # åˆ›å»ºåµŒå…¥å™¨
        embedder = EmbedderFactory.create_embedder(
            config_data['embedder_type'],
            **config_data['config']
        )
        
        # æµ‹è¯•æ•°æ®
        test_texts = [
            "é…ç½®æ–‡ä»¶è®©ç³»ç»Ÿæ›´åŠ çµæ´»å¯é…ç½®ã€‚",
            "å¤šå±‚å‘é‡åŒ–æä¾›äº†æ›´ä¸°å¯Œçš„æ–‡æœ¬è¡¨ç¤ºã€‚",
            "ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†æ˜¯ç³»ç»Ÿçš„é‡è¦åŠŸèƒ½ã€‚"
        ]
        
        vectors = embedder.encode(test_texts)
        print(f"\né…ç½®æ–‡ä»¶æµ‹è¯•æˆåŠŸ!")
        print(f"å‘é‡ç»´åº¦: {len(vectors[0])}")
        print(f"å¤„ç†æ–‡æ¡£æ•°: {len(vectors)}")
        
        return embedder
        
    except Exception as e:
        print(f"é…ç½®æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return None

def demo_large_dataset_scenario():
    """å¤§æ•°æ®é›†å¤„ç†ç¤ºä¾‹ (æ¨¡æ‹Ÿ1500ç¯‡æ–‡ç« )"""
    print("\n" + "=" * 60)
    print("å¤§æ•°æ®é›†å¤„ç†ç¤ºä¾‹ (æ¨¡æ‹Ÿåœºæ™¯)")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿ1500ç¯‡æ–‡ç« çš„åœºæ™¯
    base_texts = [
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼",
        "æœºå™¨å­¦ä¹ ç®—æ³•ä¸æ–­ä¼˜åŒ–å’Œæ”¹è¿›",
        "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—çªç ´",
        "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯æ—¥ç›Šæˆç†Ÿ",
        "æ•°æ®ç§‘å­¦å¸®åŠ©ä¼ä¸šåšå‡ºæ›´å¥½çš„å†³ç­–",
        "è®¡ç®—æœºè§†è§‰åº”ç”¨äºè‡ªåŠ¨é©¾é©¶æ±½è½¦",
        "ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼",
        "å¤§æ•°æ®åˆ†ææ­ç¤ºéšè—çš„æ¨¡å¼",
        "äº‘è®¡ç®—ä¸ºAIæä¾›å¼ºå¤§çš„è®¡ç®—èƒ½åŠ›",
        "ç‰©è”ç½‘è®¾å¤‡äº§ç”Ÿæµ·é‡æ•°æ®"
    ]
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™å°†æ˜¯çœŸå®çš„æ–‡ç« æ•°æ®ï¼‰
    print("ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›†...")
    simulated_articles = []
    for i in range(150):  # ä½¿ç”¨150ç¯‡æ–‡ç« æ¥æ¼”ç¤ºï¼Œé¿å…å¤„ç†æ—¶é—´è¿‡é•¿
        text = base_texts[i % len(base_texts)]
        simulated_articles.append(f"{text}ã€‚è¿™æ˜¯ç¬¬{i+1}ç¯‡æ–‡ç« çš„å†…å®¹ã€‚")
    
    print(f"ç”Ÿæˆäº† {len(simulated_articles)} ç¯‡æ¨¡æ‹Ÿæ–‡ç« ")
    
    # ä¼˜åŒ–çš„é…ç½®ç”¨äºå¤§æ•°æ®é›†
    large_dataset_config = [
        {
            'name': 'tfidf_statistical',
            'type': 'TFIDFEmbedder',
            'config': {
                'max_features': 2000,  # å¢åŠ ç‰¹å¾æ•°é‡
                'chinese_tokenizer': 'jieba',
                'ngram_range': (1, 3),  # åŒ…å«ä¸‰å…ƒç»„
                'min_df': 2,  # æœ€å°æ–‡æ¡£é¢‘ç‡
                'max_df': 0.8  # æœ€å¤§æ–‡æ¡£é¢‘ç‡
            }
        },
        {
            'name': 'topic_themes',
            'type': 'TopicEmbedder',
            'config': {
                'n_topics': 20,  # å¢åŠ ä¸»é¢˜æ•°é‡
                'method': 'lda',
                'language': 'chinese'
            }
        }
    ]
    
    print("\nåˆ›å»ºä¼˜åŒ–çš„å¤šå±‚åµŒå…¥å™¨...")
    large_embedder = EmbedderFactory.create_embedder(
        'MultiLayerEmbedder',
        embedder_configs=large_dataset_config,
        fusion_method='concat',
        normalize=True
    )
    
    # æ‰¹é‡å¤„ç†
    print("å¼€å§‹æ‰¹é‡å¤„ç†...")
    batch_size = 50
    all_vectors = []
    
    for i in range(0, len(simulated_articles), batch_size):
        batch = simulated_articles[i:i+batch_size]
        print(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(simulated_articles)-1)//batch_size + 1}")
        
        batch_vectors = large_embedder.encode(batch)
        all_vectors.extend(batch_vectors)
    
    print(f"\nå¤§æ•°æ®é›†å¤„ç†å®Œæˆ!")
    print(f"æ€»æ–‡æ¡£æ•°: {len(all_vectors)}")
    print(f"å‘é‡ç»´åº¦: {len(all_vectors[0])}")
    print(f"å‘é‡æ•°æ®ç±»å‹: {type(all_vectors[0][0])}")
    
    # ç®€å•çš„å‘é‡ç»Ÿè®¡
    vector_array = np.array(all_vectors)
    print(f"å‘é‡ç»Ÿè®¡ä¿¡æ¯:")
    print(f"- å‡å€¼: {np.mean(vector_array):.4f}")
    print(f"- æ ‡å‡†å·®: {np.std(vector_array):.4f}")
    print(f"- æœ€å°å€¼: {np.min(vector_array):.4f}")
    print(f"- æœ€å¤§å€¼: {np.max(vector_array):.4f}")
    
    return large_embedder, all_vectors

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¤šå±‚å‘é‡åŒ–ç³»ç»Ÿå®Œæ•´ç¤ºä¾‹")
    print("Multi-layer Vectorization System Complete Example")
    print("=" * 60)
    
    try:
        # 1. åŸºç¡€ä½¿ç”¨
        tfidf_embedder, topic_embedder = demo_basic_usage()
        
        # 2. å¤šå±‚èåˆ
        concat_embedder, weighted_embedder, attention_embedder = demo_multi_layer_fusion()
        
        # 3. é…ç½®æ–‡ä»¶ä½¿ç”¨
        config_embedder = demo_config_file_usage()
        
        # 4. å¤§æ•°æ®é›†å¤„ç†
        large_embedder, vectors = demo_large_dataset_scenario()
        
        # æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ‰ æ¼”ç¤ºå®Œæˆæ€»ç»“")
        print("=" * 60)
        print("âœ… æ‰€æœ‰åŠŸèƒ½æµ‹è¯•é€šè¿‡:")
        print("  - TF-IDF åµŒå…¥å™¨ (æ”¯æŒä¸­æ–‡åˆ†è¯)")
        print("  - ä¸»é¢˜åµŒå…¥å™¨ (LDA)")
        print("  - å¤šå±‚èåˆç­–ç•¥ (concat, weighted, attention)")
        print("  - é…ç½®æ–‡ä»¶æ”¯æŒ")
        print("  - å¤§æ•°æ®é›†æ‰¹é‡å¤„ç†")
        print("  - ä¸­æ–‡æ–‡æœ¬é¢„å¤„ç†")
        print("  - å‘é‡ç¼“å­˜å’Œä¼˜åŒ–")
        
        print("\nğŸ”§ æ¨èçš„ä¸‹ä¸€æ­¥:")
        print("  1. å®‰è£… BERTopic ç”¨äºæ›´é«˜çº§çš„ä¸»é¢˜å»ºæ¨¡: pip install bertopic")
        print("  2. ä½¿ç”¨ semantic-clustify è¿›è¡Œå‘é‡èšç±»åˆ†æ")
        print("  3. æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´é…ç½®å‚æ•°")
        print("  4. åœ¨çœŸå®æ•°æ®é›†ä¸Šæµ‹è¯•æ€§èƒ½")
        
    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print(f"âŒ é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
